{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkdphio1MyfM"
      },
      "source": [
        "## Conversation Summarization Project\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook focuses on the **fine-tuning** of Facebook’s BART model for the task of abstractive conversation summarization. Using the SAMSum dataset, we train the model to produce summaries that are concise, fluent, and semantically faithful to the content of informal chat conversations. The dataset reflects real-life messaging styles, making it ideal for building a practical chat summarizer. This fine-tuning process serves as the foundation for evaluating our custom model against a state-of-the-art LLM (Gemini) in later stages of the project."
      ],
      "metadata": {
        "id": "KWyecdPm2TYy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snDlR92_HwvU"
      },
      "source": [
        "# Install the necessary libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR5A9_OlR7si",
        "outputId": "e3574ce6-060d-47ca-ee23-c39e8b45b059",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=fe1b1edfc5a19ad539b824cfff09b5c704469eceb11edeb584f8f3c126e38687\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: fsspec, rouge_score, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 evaluate-0.4.3 fsspec-2025.3.0 rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install -U datasets fsspec evaluate rouge_score\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "import nltk\n",
        "import evaluate\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmUOv29OKEdH"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "The original dataset produced by Samsung was no longer available on Hugging Face when I started this project, so I used a copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ijZrafHveJ",
        "outputId": "50d1e28e-5c56-4700-c279-7d9f24ddcbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data = load_dataset('knkarthick/samsum')\n",
        "\n",
        "metric = evaluate.load('rouge')\n",
        "model_checkpoints = 'facebook/bart-large-xsum'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guZ_e3gNLTjd"
      },
      "source": [
        "## Data tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogmXI5cKn_jG"
      },
      "outputs": [],
      "source": [
        "max_input = 512\n",
        "max_target = 128\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0iO80XDn_nE"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(batch):\n",
        "    dialogues = batch['dialogue']\n",
        "    summaries = batch['summary']\n",
        "\n",
        "    valid_inputs = []\n",
        "    valid_targets = []\n",
        "\n",
        "\n",
        "    for d, s in zip(dialogues, summaries):\n",
        "        if isinstance(d, str) and isinstance(s, str):\n",
        "            valid_inputs.append(d)\n",
        "            valid_targets.append(s)\n",
        "\n",
        "\n",
        "    if len(valid_inputs) == 0:\n",
        "        return {\n",
        "            'input_ids': [],\n",
        "            'attention_mask': [],\n",
        "            'labels': []\n",
        "        }\n",
        "\n",
        "    # tokenize input\n",
        "    model_inputs = tokenizer(valid_inputs, max_length=max_input, padding='max_length', truncation=True)\n",
        "\n",
        "    # tokenize target\n",
        "    with tokenizer.as_target_tokenizer() if hasattr(tokenizer, \"as_target_tokenizer\") else tokenizer:\n",
        "        labels = tokenizer(valid_targets, max_length=max_target, padding='max_length', truncation=True)\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "fdc5112e43de480f80267ebb7c28f800",
            "46baf6bc48d24f7f98b78193a95d2b43",
            "061e51575a724d85a52cd5cd8a7c16d4",
            "720309dd130244e8802bd1c692fbb742",
            "62b43f08b16e48db9ef3b8f658631a55",
            "586289f22b5b4a35803b0c624abc9bed",
            "b0955127af7a4d86a972fbcabafe7be3",
            "638e33ac6c0245aaa95fd898bcb786c3",
            "c7637c9524ec467ebd5bdd8366ec6933",
            "2b834c8751a44c438e121705403a103f",
            "86ee9ed157a94cc3a980cfab5f14f1bf"
          ]
        },
        "id": "rygqQ1Gkn_qu",
        "outputId": "3d3a9f8f-101a-4984-fef3-0be46052d009"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdc5112e43de480f80267ebb7c28f800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenize_data = data.map(\n",
        "    preprocess_data,\n",
        "    batched=True,\n",
        "    remove_columns=['id', 'dialogue', 'summary']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvMOVIBiMpOo"
      },
      "source": [
        "## Sampling the dataset\n",
        "\n",
        "To perform a quick test run before full training, we randomly sampled 1000 training examples, 500 validation examples, and 200 test examples from the SAMSum dataset. This reduced subset allowed faster experimentation. The final fine-tuning, however, was carried out on the entire dataset to ensure optimal model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWACCTpoGu8b"
      },
      "outputs": [],
      "source": [
        "#sample the data\n",
        "train_sample = tokenize_data['train'].shuffle(seed=123).select(range(1000))\n",
        "validation_sample = tokenize_data['validation'].shuffle(seed=123).select(range(500))\n",
        "test_sample = tokenize_data['test'].shuffle(seed=123).select(range(200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tKiNnNUHBS6"
      },
      "outputs": [],
      "source": [
        "tokenize_data['train'] = train_sample\n",
        "tokenize_data['validation'] = validation_sample\n",
        "tokenize_data['test'] = test_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4TZUupJHFWY",
        "outputId": "7fdf60c3-56a4-44f0-b9fa-924b8801159f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenize_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AYj3_LNL2mk"
      },
      "source": [
        "## Training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3bEO3dOpCT8"
      },
      "outputs": [],
      "source": [
        "#load model\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFoZ3uwUpCX7"
      },
      "outputs": [],
      "source": [
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s8_pdYopCbU"
      },
      "outputs": [],
      "source": [
        "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4mwnHjvpCeT"
      },
      "outputs": [],
      "source": [
        "# metrics\n",
        "\n",
        "def compute_rouge(pred):\n",
        "  predictions, labels = pred\n",
        "  #decode the predictions\n",
        "  decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  #decode labels\n",
        "  decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  #compute results\n",
        "  res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
        "  #get %\n",
        "  res = {key: value * 100 for key, value in res.items()}\n",
        "\n",
        "  pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  res['gen_len'] = np.mean(pred_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in res.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNGxpixQqUQM"
      },
      "outputs": [],
      "source": [
        "args = transformers.Seq2SeqTrainingArguments(\n",
        "    output_dir = 'conversation-summ',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size= 1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=2,\n",
        "    predict_with_generate=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    report_to = 'none'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN4P1UsjqUTq",
        "outputId": "3cdae10d-3bfe-462b-ac1c-40f275b50525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3425184666>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = transformers.Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenize_data['train'],\n",
        "    eval_dataset=tokenize_data['validation'],\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_rouge\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "T_tX926lqUXS",
        "outputId": "e97f1ab4-764a-476f-f60e-3e5839be90e3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14732' max='14732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14732/14732 2:06:17, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.308600</td>\n",
              "      <td>0.303632</td>\n",
              "      <td>53.331100</td>\n",
              "      <td>28.748700</td>\n",
              "      <td>44.169500</td>\n",
              "      <td>44.114600</td>\n",
              "      <td>27.515900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.215700</td>\n",
              "      <td>0.316653</td>\n",
              "      <td>54.527800</td>\n",
              "      <td>30.179400</td>\n",
              "      <td>44.986200</td>\n",
              "      <td>44.990100</td>\n",
              "      <td>29.183400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14732, training_loss=0.2766784338273984, metrics={'train_runtime': 7578.8014, 'train_samples_per_second': 3.887, 'train_steps_per_second': 1.944, 'total_flos': 3.192361789371187e+16, 'train_loss': 0.2766784338273984, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/conversation-summ /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs_j_7pLlx3k",
        "outputId": "4878dbc6-7847-41f8-dfb8-948c232daadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r conversation-summ.zip /content/conversation-summ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVGZjUXlmUR5",
        "outputId": "aee34285-d705-415e-8979-7589c04d7f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/conversation-summ/ (stored 0%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/ (stored 0%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/training_args.bin (deflated 51%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/model.safetensors (deflated 7%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/config.json (deflated 61%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/merges.txt (deflated 53%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/trainer_state.json (deflated 76%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/rng_state.pth (deflated 25%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/scheduler.pt (deflated 55%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/scaler.pt (deflated 60%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/vocab.json (deflated 59%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/optimizer.pt (deflated 9%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/generation_config.json (deflated 43%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/tokenizer.json (deflated 82%)\n",
            "  adding: content/conversation-summ/checkpoint-14732/tokenizer_config.json (deflated 76%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/ (stored 0%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/training_args.bin (deflated 51%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/model.safetensors (deflated 7%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/config.json (deflated 61%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/merges.txt (deflated 53%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/trainer_state.json (deflated 75%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/rng_state.pth (deflated 25%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/scheduler.pt (deflated 56%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/scaler.pt (deflated 60%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/vocab.json (deflated 59%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/optimizer.pt (deflated 9%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/generation_config.json (deflated 43%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/tokenizer.json (deflated 82%)\n",
            "  adding: content/conversation-summ/checkpoint-7366/tokenizer_config.json (deflated 76%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "AvMOVIBiMpOo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fdc5112e43de480f80267ebb7c28f800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46baf6bc48d24f7f98b78193a95d2b43",
              "IPY_MODEL_061e51575a724d85a52cd5cd8a7c16d4",
              "IPY_MODEL_720309dd130244e8802bd1c692fbb742"
            ],
            "layout": "IPY_MODEL_62b43f08b16e48db9ef3b8f658631a55"
          }
        },
        "46baf6bc48d24f7f98b78193a95d2b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586289f22b5b4a35803b0c624abc9bed",
            "placeholder": "​",
            "style": "IPY_MODEL_b0955127af7a4d86a972fbcabafe7be3",
            "value": "Map: 100%"
          }
        },
        "061e51575a724d85a52cd5cd8a7c16d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_638e33ac6c0245aaa95fd898bcb786c3",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7637c9524ec467ebd5bdd8366ec6933",
            "value": 819
          }
        },
        "720309dd130244e8802bd1c692fbb742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b834c8751a44c438e121705403a103f",
            "placeholder": "​",
            "style": "IPY_MODEL_86ee9ed157a94cc3a980cfab5f14f1bf",
            "value": " 819/819 [00:01&lt;00:00, 778.77 examples/s]"
          }
        },
        "62b43f08b16e48db9ef3b8f658631a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586289f22b5b4a35803b0c624abc9bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0955127af7a4d86a972fbcabafe7be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "638e33ac6c0245aaa95fd898bcb786c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7637c9524ec467ebd5bdd8366ec6933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b834c8751a44c438e121705403a103f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ee9ed157a94cc3a980cfab5f14f1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}